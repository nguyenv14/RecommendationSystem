# üîß FIX ERRORS - H∆∞·ªõng D·∫´n S·ª≠a L·ªói

## ‚ùå L·ªói 1: Qdrant Constructor

**L·ªói:**
```
TypeError: __init__() got an unexpected keyword argument 'url'
```

**ƒê√£ s·ª≠a:** ‚úÖ 
- D√πng `QdrantClient` tr∆∞·ªõc, sau ƒë√≥ pass v√†o `Qdrant()`
- S·ª≠a trong `load_vectorstore()` method

---

## ‚ùå L·ªói 2: Ollama Embedding API

**L·ªói:**
```
ValueError: Error raised by inference API HTTP code: 500
{"error":"do embedding request: Post \"http://127.0.0.1:53228/embedding\": EOF"}
```

**Nguy√™n nh√¢n:**
- Model `bge-m3` ch∆∞a ƒë∆∞·ª£c pull trong Ollama
- Ho·∫∑c Ollama ch∆∞a ƒë√∫ng c·∫•u h√¨nh
- Ho·∫∑c embedding API kh√¥ng ho·∫°t ƒë·ªông

---

## üîß C√ÅCH S·ª¨A L·ªñI OLLAMA

### **B∆∞·ªõc 1: Check Ollama Models**

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Ho·∫∑c d√πng script
python3 check_ollama.py
```

### **B∆∞·ªõc 2: Pull Required Models**

```bash
# Pull embedding model
ollama pull bge-m3

# Pull LLM model
ollama pull llama2

# Ho·∫∑c n·∫øu kh√¥ng c√≥, d√πng model kh√°c
ollama pull mistral
```

### **B∆∞·ªõc 3: Verify Models**

```bash
# List models
ollama list

# Test embedding
curl http://localhost:11434/api/embeddings \
  -d '{"model": "bge-m3", "prompt": "test"}'
```

### **B∆∞·ªõc 4: Test v·ªõi Script**

```bash
python3 check_ollama.py
```

---

## üîÑ **ALTERNATIVE: D√πng Model Kh√°c**

N·∫øu `bge-m3` kh√¥ng c√≥, c√≥ th·ªÉ d√πng model kh√°c:

### **Option 1: D√πng Sentence Transformers (Local)**

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

# Instead of OllamaEmbeddings
embeddings = HuggingFaceEmbeddings(
    model_name="paraphrase-multilingual-MiniLM-L12-v2"
)
```

### **Option 2: D√πng Model Ollama Kh√°c**

```python
# Th·ª≠ model kh√°c
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",  # Thay v√¨ bge-m3
    base_url="http://localhost:11434"
)
```

**Pull model:**
```bash
ollama pull nomic-embed-text
```

---

## ‚úÖ **CHECKLIST FIX**

- [ ] Ollama is running
- [ ] Model `bge-m3` pulled: `ollama pull bge-m3`
- [ ] Model `llama2` pulled: `ollama pull llama2`
- [ ] Test embedding API: `python3 check_ollama.py`
- [ ] Qdrant running: `docker-compose up -d`
- [ ] RAG system ch·∫°y th√†nh c√¥ng

---

## üöÄ **QUICK FIX**

```bash
# 1. Pull models
ollama pull bge-m3
ollama pull llama2

# 2. Check models
python3 check_ollama.py

# 3. Run RAG
python3 simple_rag_system.py
```

---

**TL;DR**: 
1. `ollama pull bge-m3`
2. `ollama pull llama2`
3. `python3 check_ollama.py`
4. `python3 simple_rag_system.py`

