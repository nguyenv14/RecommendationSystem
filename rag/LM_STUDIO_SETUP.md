# H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng LM Studio v·ªõi RAG System

## üìã C·∫•u h√¨nh

- **LM Studio URL**: `http://192.168.10.42:1234`
- **Model**: `qwen/qwen3-4b-2507`
- **Embeddings**: V·∫´n s·ª≠ d·ª•ng Ollama (`bge-m3`)

## üöÄ C√°ch s·ª≠ d·ª•ng

### Option 1: S·ª≠ d·ª•ng script t·ª± ƒë·ªông

```bash
cd rag
./run_with_lm_studio.sh
```

Script n√†y s·∫Ω:
- ‚úÖ Ki·ªÉm tra LM Studio ƒëang ch·∫°y
- ‚úÖ Ki·ªÉm tra Ollama (cho embeddings)
- ‚úÖ Ki·ªÉm tra Qdrant
- ‚úÖ Kh·ªüi ƒë·ªông Flask API v·ªõi c·∫•u h√¨nh LM Studio

### Option 2: S·ª≠ d·ª•ng environment variables

```bash
cd rag
source venv_rag/bin/activate

export LLM_PROVIDER="lm_studio"
export LM_STUDIO_URL="http://192.168.10.42:1234"
export LLM_MODEL="qwen/qwen3-4b-2507"
export OLLAMA_URL="http://localhost:11434"  # For embeddings
export QDRANT_URL="http://localhost:6333"

python rag_chat_api.py
```

### Option 3: S·ª≠ d·ª•ng script run_chat.sh

```bash
cd rag
source venv_rag/bin/activate

export LLM_PROVIDER="lm_studio"
export LM_STUDIO_URL="http://192.168.10.42:1234"
export LLM_MODEL="qwen/qwen3-4b-2507"

./run_chat.sh
```

## üß™ Test connection

Test LM Studio connection:

```bash
cd rag
source venv_rag/bin/activate

export LM_STUDIO_URL="http://192.168.10.42:1234"
export LLM_MODEL="qwen/qwen3-4b-2507"

python test_lm_studio.py
```

## ‚öôÔ∏è C·∫•u h√¨nh trong code

B·∫°n c√≥ th·ªÉ c·∫•u h√¨nh tr·ª±c ti·∫øp trong code:

```python
from simple_rag_system import SimpleRAGSystem

rag_system = SimpleRAGSystem(
    ollama_url="http://localhost:11434",  # For embeddings
    qdrant_url="http://localhost:6333",
    embedding_model="bge-m3",
    llm_model="qwen/qwen3-4b-2507",
    collection_name="hotels",
    llm_provider="lm_studio",  # Use LM Studio
    lm_studio_url="http://192.168.10.42:1234"
)
```

## üìù L∆∞u √Ω

1. **LM Studio ph·∫£i ƒëang ch·∫°y**: ƒê·∫£m b·∫£o LM Studio ƒëang ch·∫°y v√† model `qwen/qwen3-4b-2507` ƒë√£ ƒë∆∞·ª£c load
2. **Ollama v·∫´n c·∫ßn cho embeddings**: H·ªá th·ªëng v·∫´n s·ª≠ d·ª•ng Ollama ƒë·ªÉ t·∫°o embeddings, ch·ªâ LLM d√πng LM Studio
3. **Qdrant ph·∫£i ch·∫°y**: Vector database Qdrant ph·∫£i ƒëang ch·∫°y
4. **Model name ch√≠nh x√°c**: Model name ph·∫£i kh·ªõp v·ªõi t√™n trong LM Studio (c√≥ th·ªÉ l√† `qwen/qwen3-4b-2507` ho·∫∑c `qwen3-4b-2507`)

## üîç Troubleshooting

### LM Studio kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c

```bash
# Test connection
curl http://192.168.10.42:1234/v1/models

# Test model
curl -X POST http://192.168.10.42:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen/qwen3-4b-2507",
    "messages": [{"role": "user", "content": "test"}]
  }'
```

### Model kh√¥ng t√¨m th·∫•y

- Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load trong LM Studio ch∆∞a
- Ki·ªÉm tra t√™n model ch√≠nh x√°c (c√≥ th·ªÉ c·∫ßn `qwen/qwen3-4b-2507` thay v√¨ `qwen3-4b-2507`)

### L·ªói ChatOpenAI initialization

- Ki·ªÉm tra LangChain version: `pip install --upgrade langchain-community`
- Th·ª≠ v·ªõi `openai_api_base` thay v√¨ `base_url` (code ƒë√£ c√≥ fallback)

## üéØ So s√°nh v·ªõi Ollama

| Feature | Ollama | LM Studio |
|---------|--------|-----------|
| **Setup** | D·ªÖ d√†ng | C·∫ßn c√†i LM Studio |
| **Models** | Nhi·ªÅu models | Models t·ª´ Hugging Face |
| **Performance** | CPU optimized | GPU/CPU support |
| **API Format** | Ollama API | OpenAI-compatible |
| **Embeddings** | C√≥ s·∫µn | C·∫ßn setup ri√™ng |

## üí° L·ª£i √≠ch c·ªßa LM Studio

1. **GUI d·ªÖ s·ª≠ d·ª•ng**: Qu·∫£n l√Ω models qua giao di·ªán ƒë·ªì h·ªça
2. **Nhi·ªÅu models**: D·ªÖ d√†ng t·∫£i v√† chuy·ªÉn ƒë·ªïi models t·ª´ Hugging Face
3. **GPU support**: H·ªó tr·ª£ GPU t·ªët h∆°n (n·∫øu c√≥)
4. **OpenAI-compatible**: API t∆∞∆°ng th√≠ch v·ªõi OpenAI, d·ªÖ t√≠ch h·ª£p

