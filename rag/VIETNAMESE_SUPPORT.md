# ğŸ‡»ğŸ‡³ Há»— Trá»£ Tiáº¿ng Viá»‡t cho RAG System

## ğŸ¯ Váº¥n Äá»

Model `llama2` khÃ´ng há»— trá»£ tiáº¿ng Viá»‡t tá»‘t, dáº«n Ä‘áº¿n cÃ¢u tráº£ lá»i láº«n lá»™n giá»¯a tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t.

## âœ… Giáº£i PhÃ¡p

### 1. **Sá»­ dá»¥ng Model Há»— Trá»£ Tiáº¿ng Viá»‡t Tá»‘t HÆ¡n**

ÄÃ£ cáº­p nháº­t default model tá»« `llama2` sang `qwen3` vÃ¬:
- âœ… Qwen3 há»— trá»£ tiáº¿ng Viá»‡t ráº¥t tá»‘t (model cá»§a Alibaba)
- âœ… Hiá»ƒu ngá»¯ cáº£nh tiáº¿ng Viá»‡t tá»‘t hÆ¡n
- âœ… Tráº£ lá»i nháº¥t quÃ¡n vÃ  tá»± nhiÃªn hÆ¡n
- âœ… ÄÃ£ cÃ³ sáºµn trong Ollama cá»§a báº¡n

### 2. **Cáº£i Thiá»‡n Prompt Template**

ÄÃ£ cáº­p nháº­t prompt template vá»›i:
- YÃªu cáº§u rÃµ rÃ ng: "PHáº¢I tráº£ lá»i HOÃ€N TOÃ€N báº±ng tiáº¿ng Viá»‡t"
- Nháº¥n máº¡nh nhiá»u láº§n vá» yÃªu cáº§u tiáº¿ng Viá»‡t
- HÆ°á»›ng dáº«n chi tiáº¿t vá» cÃ¡ch tráº£ lá»i

### 3. **Äiá»u Chá»‰nh Temperature**

Giáº£m temperature tá»« `0.7` xuá»‘ng `0.3` Ä‘á»ƒ:
- CÃ¢u tráº£ lá»i táº­p trung hÆ¡n
- Giáº£m sá»± láº«n lá»™n ngÃ´n ngá»¯
- Tráº£ lá»i nháº¥t quÃ¡n hÆ¡n

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### **BÆ°á»›c 1: Kiá»ƒm Tra Model Qwen3**

```bash
# Kiá»ƒm tra model Ä‘Ã£ cÃ³
ollama list

# Náº¿u chÆ°a cÃ³ qwen3, pull model:
ollama pull qwen3
```

### **BÆ°á»›c 2: Cháº¡y Test**

```bash
cd rag
python test_rag.py
```

Hoáº·c:

```bash
python simple_rag_system.py
```

## ğŸ“ CÃ¡c Model KhÃ¡c Há»— Trá»£ Tiáº¿ng Viá»‡t

Náº¿u `mistral` váº«n chÆ°a Ä‘á»§ tá»‘t, cÃ³ thá»ƒ thá»­:

### **Option 1: Llama3**
```bash
ollama pull llama3
```

Cáº­p nháº­t trong code:
```python
llm_model="llama3"
```

### **Option 2: Phi3** (Nháº¹, nhanh)
```bash
ollama pull phi3
```

Cáº­p nháº­t trong code:
```python
llm_model="phi3"
```

### **Option 3: Gemma** (Google)
```bash
ollama pull gemma
```

Cáº­p nháº­t trong code:
```python
llm_model="gemma"
```

## ğŸ”§ Cáº¥u HÃ¬nh NÃ¢ng Cao

### **Thay Äá»•i Model**

Trong `simple_rag_system.py` hoáº·c khi khá»Ÿi táº¡o:

```python
rag = SimpleRAGSystem(
    ollama_url="http://localhost:11434",
    qdrant_url="http://localhost:6333",
    embedding_model="bge-m3",
    llm_model="mistral"  # Thay Ä‘á»•i model á»Ÿ Ä‘Ã¢y
)
```

### **Äiá»u Chá»‰nh Temperature**

Náº¿u cáº§n Ä‘iá»u chá»‰nh Ä‘á»™ sÃ¡ng táº¡o cá»§a cÃ¢u tráº£ lá»i:

```python
self.llm = Ollama(
    model=llm_model,
    base_url=ollama_url,
    temperature=0.3  # 0.0-1.0: Tháº¥p = táº­p trung, Cao = sÃ¡ng táº¡o
)
```

## ğŸ“Š So SÃ¡nh Models

| Model | Tiáº¿ng Viá»‡t | Tá»‘c Äá»™ | KÃ­ch ThÆ°á»›c | Äá» Xuáº¥t |
|-------|------------|--------|------------|---------|
| llama2 | â­â­ | âš¡âš¡âš¡ | 3.8GB | âŒ KhÃ´ng |
| qwen3 | â­â­â­â­â­ | âš¡âš¡âš¡ | 5.2GB | âœ… **Máº·c Ä‘á»‹nh** |
| mistral | â­â­â­â­ | âš¡âš¡âš¡ | 4.1GB | âœ… Tá»‘t |
| llama3 | â­â­â­â­ | âš¡âš¡ | 4.7GB | âœ… Tá»‘t |
| phi3 | â­â­â­ | âš¡âš¡âš¡âš¡ | 2.3GB | âœ… Nhanh |
| gemma | â­â­â­ | âš¡âš¡âš¡ | 2.0GB | âœ… Nháº¹ |

## âœ… Checklist

- [ ] Kiá»ƒm tra model qwen3: `ollama list` (nÃªn cÃ³ sáºµn)
- [ ] Náº¿u chÆ°a cÃ³, pull model: `ollama pull qwen3`
- [ ] Cháº¡y test: `python test_rag.py`
- [ ] Kiá»ƒm tra cÃ¢u tráº£ lá»i cÃ³ hoÃ n toÃ n báº±ng tiáº¿ng Viá»‡t khÃ´ng
- [ ] Náº¿u chÆ°a tá»‘t, thá»­ model khÃ¡c (mistral, llama3, phi3, gemma)

## ğŸ› Troubleshooting

### **Lá»—i: Model khÃ´ng tÃ¬m tháº¥y**
```bash
# Kiá»ƒm tra Ollama Ä‘ang cháº¡y
curl http://localhost:11434/api/tags

# Pull model qwen3
ollama pull qwen3
```

### **Lá»—i: CÃ¢u tráº£ lá»i váº«n láº«n lá»™n**
1. Kiá»ƒm tra prompt template Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t chÆ°a
2. Thá»­ model khÃ¡c (llama3, phi3)
3. Giáº£m temperature xuá»‘ng 0.1-0.2

### **Lá»—i: Tráº£ lá»i quÃ¡ cháº­m**
- Thá»­ model nháº¹ hÆ¡n: `phi3` hoáº·c `gemma`
- Giáº£m `k` trong retriever (sá»‘ documents retrieved)

---

**LÆ°u Ã½:** Model `qwen3` lÃ  default má»›i vÃ  Ä‘Ã£ cÃ³ sáºµn trong Ollama cá»§a báº¡n. Qwen3 há»— trá»£ tiáº¿ng Viá»‡t ráº¥t tá»‘t!

