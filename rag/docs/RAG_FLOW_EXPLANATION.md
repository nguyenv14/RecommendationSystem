# Giáº£i thÃ­ch RAG System vÃ  Query Flow

## ğŸ“‹ Tá»•ng quan RAG (Retrieval-Augmented Generation)

RAG = **Retrieval** (TÃ¬m kiáº¿m) + **Augmented** (Bá»• sung) + **Generation** (Táº¡o cÃ¢u tráº£ lá»i)

RAG system nÃ y hoáº¡t Ä‘á»™ng theo 2 mode:
1. **Semantic Search** (`search_hotels()`) - Chá»‰ tÃ¬m kiáº¿m, khÃ´ng dÃ¹ng LLM
2. **RAG Query** (`ask()`) - TÃ¬m kiáº¿m + LLM generation Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i

---

## ğŸ” 1. SEMANTIC SEARCH FLOW (`search_hotels()`)

### Flow diagram:
```
User Query
    â†“
[1] Extract location (optional)
    â†“
[2] Generate Query Embedding (Ollama bge-m3)
    â†“
[3] Vector Search in Qdrant
    â”œâ”€ With location filter (if provided)
    â””â”€ Without filter (general search)
    â†“
[4] Similarity Scoring & Ranking
    â†“
[5] Format Results (hotel metadata)
    â†“
Return List[Dict] of hotels
```

### Code Flow:

#### BÆ°á»›c 1: Extract Location (Optional)
```python
# Line 800-802
if area_name is None:
    area_name = self._extract_location_from_query(query)
# Extract location tá»« query: "SÆ¡n TrÃ ", "NgÅ© HÃ nh SÆ¡n", etc.
```

#### BÆ°á»›c 2: Generate Query Embedding
```python
# Line 815 (vá»›i filter) hoáº·c Line 874 (khÃ´ng filter
query_embedding = self.embeddings.embed_query(query)
# â†’ Vector 1024 dimensions tá»« Ollama bge-m3 model
# â†’ Cached Ä‘á»ƒ tá»‘i Æ°u performance
```

#### BÆ°á»›c 3: Vector Search trong Qdrant

**Option A: Vá»›i Location Filter**
```python
# Line 819-830
search_results = client.search(
    collection_name=self.collection_name,
    query_vector=query_embedding,
    limit=min(top_k + 1, 5),  # Top 5 results
    query_filter=Filter(
        must=[
            FieldCondition(key="area_name", match=MatchValue(value=area_name))
        ]
    ),
    with_payload=True,
    with_vectors=False  # KhÃ´ng cáº§n vectors trong response
)
```

**Option B: KhÃ´ng cÃ³ Filter (General Search)**
```python
# Line 874-876
results = self.vectorstore.similarity_search_with_score(
    query,
    k=min(top_k + 1, 5)  # Top 5 results
)
```

#### BÆ°á»›c 4: Similarity Scoring
```python
# Line 886-887 (Cosine Distance â†’ Similarity)
similarity_score = max(0, 1 - score)  # Normalize to [0, 1]
# Filter: chá»‰ láº¥y similarity > 0.3 (Line 891)
```

#### BÆ°á»›c 5: Format Results
```python
# Line 905-915
hotels.append({
    "hotel_id": doc.metadata.get("hotel_id"),
    "hotel_name": hotel_name,
    "hotel_rank": doc.metadata.get("hotel_rank"),
    "hotel_price_average": doc.metadata.get("hotel_price_average"),
    "area_name": doc.metadata.get("area_name", ""),
    "similarity_score": float(similarity_score),
    "text_preview": doc.page_content[:200] + "..."
})
```

### Káº¿t quáº£:
- Tráº£ vá» `List[Dict]` vá»›i top_k hotels
- Má»—i hotel cÃ³ metadata: ID, tÃªn, giÃ¡, Ä‘Ã¡nh giÃ¡, similarity score
- **KhÃ´ng cÃ³ LLM generation** - chá»‰ vector search

---

## ğŸ’¬ 2. RAG QUERY FLOW (`ask()`)

### Flow diagram:
```
User Question
    â†“
[1] Generate Query Embedding
    â†“
[2] Vector Search (Retriever) â†’ Top 5 documents
    â†“
[3] Combine Context tá»« 5 documents
    â†“
[4] Build Prompt vá»›i Context + Question
    â†“
[5] LLM Generation (LM Studio qwen3-4b-2507)
    â”œâ”€ max_tokens: 2048
    â”œâ”€ temperature: 0.3
    â””â”€ Prompt: Chi tiáº¿t, so sÃ¡nh hotels
    â†“
[6] Parse Response + Extract Sources
    â†“
Return Dict {answer, sources}
```

### Code Flow:

#### BÆ°á»›c 1-2: Retrieval (TÃ¬m kiáº¿m relevant documents)
```python
# Line 939
result = self.qa_chain({"query": question})
# â†’ qa_chain tá»± Ä‘á»™ng:
#    1. Generate query embedding
#    2. Search top k=5 documents trong vectorstore
#    3. Combine documents thÃ nh context
```

#### BÆ°á»›c 3-4: Context Preparation & Prompt Building
```python
# Line 708-718 (Prompt Template)
prompt_template = """Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n khÃ¡ch sáº¡n táº¡i ÄÃ  Náºµng...

ThÃ´ng tin khÃ¡ch sáº¡n:
{context}  # â† 5 documents Ä‘Æ°á»£c combine á»Ÿ Ä‘Ã¢y

CÃ¢u há»i: {question}

Tráº£ lá»i chi tiáº¿t... So sÃ¡nh cÃ¡c khÃ¡ch sáº¡n náº¿u cÃ³ nhiá»u lá»±a chá»n...
"""
```

**Context tá»« 5 documents:**
- Má»—i document lÃ  1 chunk cá»§a hotel data
- LangChain tá»± Ä‘á»™ng combine: `doc1.page_content + doc2.page_content + ...`
- Tá»•ng context cÃ³ thá»ƒ ~4000-5000 characters (vá»›i k=5, chunk_size=800)

#### BÆ°á»›c 5: LLM Generation
```python
# Line 727-734 (QA Chain)
self.qa_chain = RetrievalQA.from_chain_type(
    llm=self.llm,  # ChatOpenAI vá»›i LM Studio
    chain_type="stuff",  # Combine táº¥t cáº£ context vÃ o 1 prompt
    retriever=self.retriever,  # k=5
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)

# LLM Config:
# - max_tokens: 2048 (Line 173, 188)
# - temperature: 0.3 (Line 172, 187)
# - timeout: 120s (Line 175, 190)
```

**Chain Type = "stuff":**
- Combine táº¥t cáº£ 5 documents vÃ o 1 prompt
- LLM xá»­ lÃ½ toÃ n bá»™ context 1 láº§n
- Nhanh hÆ¡n "refine" hoáº·c "map_reduce"

#### BÆ°á»›c 6: Parse Response & Extract Sources
```python
# Line 942-965
response = {
    "question": question,
    "answer": result["result"],  # LLM generated answer
    "sources": []
}

# Extract source documents
for doc in result.get("source_documents", []):
    response["sources"].append({
        "hotel_id": doc.metadata.get("hotel_id"),
        "hotel_name": doc.metadata.get("hotel_name", ""),
        "hotel_rank": doc.metadata.get("hotel_rank"),
        "hotel_price_average": doc.metadata.get("hotel_price_average"),
        "area_name": doc.metadata.get("area_name", ""),
        "text_preview": page_content[:300] + "..."
    })
```

### Káº¿t quáº£:
- Tráº£ vá» `Dict` vá»›i:
  - `answer`: CÃ¢u tráº£ lá»i Ä‘Æ°á»£c LLM generate (tá»‘i Ä‘a 2048 tokens)
  - `sources`: List 5 hotels Ä‘Æ°á»£c dÃ¹ng lÃ m context

---

## ğŸ”„ 3. RETRIEVER CONFIGURATION

### Retriever Setup:
```python
# Line 702-706 (_initialize_qa_chain)
self.retriever = self.vectorstore.as_retriever(
    search_kwargs={
        "k": 5  # Top 5 documents
    }
)
```

**Retriever hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o:**
1. Nháº­n query text
2. Tá»± Ä‘á»™ng generate embedding qua `self.embeddings`
3. Search trong Qdrant vá»›i vector similarity
4. Tráº£ vá» top k=5 documents cÃ³ similarity cao nháº¥t
5. Combine documents thÃ nh context string

---

## ğŸ“Š 4. COMPARISON: Search vs RAG

| Feature | `search_hotels()` | `ask()` |
|---------|------------------|---------|
| **LLM** | âŒ KhÃ´ng dÃ¹ng | âœ… DÃ¹ng (LM Studio) |
| **Output** | List hotels | CÃ¢u tráº£ lá»i tá»± nhiÃªn |
| **Speed** | ~1-2s | ~5-15s |
| **Use case** | TÃ¬m danh sÃ¡ch hotels | Há»i Ä‘Ã¡p tá»± nhiÃªn |
| **Sources** | top_k hotels | top 5 hotels |
| **Context** | KhÃ´ng cÃ³ | CÃ³ (5 documents combined) |

### VÃ­ dá»¥:

**Search:**
```python
results = rag.search_hotels("KhÃ¡ch sáº¡n 5 sao gáº§n biá»ƒn")
# Returns: [{hotel_name: "...", price: ..., ...}, ...]
```

**RAG:**
```python
response = rag.ask("KhÃ¡ch sáº¡n nÃ o 5 sao gáº§n biá»ƒn?")
# Returns: {
#   answer: "Dá»±a trÃªn thÃ´ng tin tÃ¬m Ä‘Æ°á»£c, cÃ³ má»™t sá»‘ khÃ¡ch sáº¡n 5 sao gáº§n biá»ƒn...",
#   sources: [{hotel_id: 1, hotel_name: "...", ...}, ...]
# }
```

---

## ğŸ”§ 5. KEY COMPONENTS

### A. Embeddings (bge-m3)
```python
# Line 155-160
base_embeddings = OllamaEmbeddings(model="bge-m3", base_url=ollama_url)
self.embeddings = CachedOllamaEmbeddings(base_embeddings, cache_enabled=True)
```
- **Model**: bge-m3 (BAAI General Embedding)
- **Dimension**: 1024
- **Cache**: CÃ³ cache Ä‘á»ƒ tá»‘i Æ°u (trÃ¡nh re-embedding)

### B. Vector Store (Qdrant)
```python
# Line 590-594
self.vectorstore = Qdrant(
    client=client,
    collection_name=self.collection_name,
    embeddings=self.embeddings
)
```
- **Distance**: Cosine
- **Index**: HNSW (m=16, ef_construct=200)
- **Storage**: Hotel documents vá»›i metadata

### C. LLM (qwen3-4b-2507 via LM Studio)
```python
# Line 168-177
self.llm = ChatOpenAI(
    model="qwen/qwen3-4b-2507",
    openai_api_base="http://192.168.10.42:1234/v1",
    max_tokens=2048,
    temperature=0.3
)
```
- **Model**: Qwen3-4B
- **Max tokens**: 2048 (tÄƒng tá»« 512)
- **Temperature**: 0.3 (focused, consistent)

---

## ğŸ¯ 6. OPTIMIZATION FEATURES

### 1. Embedding Cache
```python
# Line 82-113
def embed_documents(self, texts: List[str]) -> List[List[float]]:
    # Check cache trÆ°á»›c khi embed
    # Cache miss má»›i gá»i Ollama
```

### 2. Batch Processing
```python
# Line 600-688
# Process documents theo batch (batch_size=50)
# Retry logic náº¿u lá»—i
```

### 3. Smart Chunking
```python
# Line 466-467
chunks = self.chunker.chunk_hotel_document(hotel_data, semantic_text)
# Chunk hotel data Ä‘á»ƒ preserve semantic meaning
```

### 4. HNSW Index
```python
# Line 572-586
hnsw_config = HnswConfigDiff(m=16, ef_construct=200)
# Fast approximate nearest neighbor search
```

---

## ğŸ“ 7. EXAMPLE QUERY FLOW

### Query: "KhÃ¡ch sáº¡n nÃ o cÃ³ view biá»ƒn Ä‘áº¹p á»Ÿ NgÅ© HÃ nh SÆ¡n?"

#### Step 1: Query Processing
```python
# Extract location
area_name = "NgÅ© HÃ nh SÆ¡n"  # Tá»« _extract_location_from_query()
```

#### Step 2: Embedding
```python
query_embedding = embed("KhÃ¡ch sáº¡n nÃ o cÃ³ view biá»ƒn Ä‘áº¹p á»Ÿ NgÅ© HÃ nh SÆ¡n?")
# â†’ [0.123, -0.456, ..., 0.789] (1024 dims)
```

#### Step 3: Vector Search (RAG mode)
```python
# Retriever search vá»›i k=5
documents = retriever.get_relevant_documents(query)
# â†’ [
#   Document(page_content="Sheraton ÄÃ  Náºµng... view biá»ƒn...", metadata={hotel_id: 1, ...}),
#   Document(page_content="InterContinental... hÆ°á»›ng biá»ƒn...", metadata={hotel_id: 2, ...}),
#   ... (5 documents total)
# ]
```

#### Step 4: Context Building
```python
context = """
Document 1: Sheraton ÄÃ  Náºµng... view biá»ƒn Ä‘áº¹p... 5 sao... 2.026.580 VND...
Document 2: InterContinental ÄÃ  Náºµng... hÆ°á»›ng biá»ƒn... 5 sao... 2.625.000 VND...
...
"""
```

#### Step 5: Prompt to LLM
```python
prompt = """
Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n khÃ¡ch sáº¡n táº¡i ÄÃ  Náºµng...

ThÃ´ng tin khÃ¡ch sáº¡n:
[context tá»« step 4]

CÃ¢u há»i: KhÃ¡ch sáº¡n nÃ o cÃ³ view biá»ƒn Ä‘áº¹p á»Ÿ NgÅ© HÃ nh SÆ¡n?

Tráº£ lá»i chi tiáº¿t, tá»± nhiÃªn báº±ng tiáº¿ng Viá»‡t...
"""
```

#### Step 6: LLM Generation
```python
# LM Studio generate response (max 2048 tokens)
answer = "Dá»±a trÃªn thÃ´ng tin tÃ¬m Ä‘Æ°á»£c, cÃ³ má»™t sá»‘ khÃ¡ch sáº¡n cÃ³ view biá»ƒn Ä‘áº¹p á»Ÿ NgÅ© HÃ nh SÆ¡n:

1. **Sheraton ÄÃ  Náºµng** - KhÃ¡ch sáº¡n 5 sao vá»›i view biá»ƒn tuyá»‡t Ä‘áº¹p, giÃ¡ trung bÃ¬nh 2.026.580 VND...

2. **InterContinental ÄÃ  Náºµng** - KhÃ¡ch sáº¡n 5 sao hÆ°á»›ng biá»ƒn, giÃ¡ trung bÃ¬nh 2.625.000 VND...

[So sÃ¡nh vÃ  tÆ° váº¥n thÃªm...]"
```

#### Step 7: Response
```python
{
    "question": "KhÃ¡ch sáº¡n nÃ o cÃ³ view biá»ƒn Ä‘áº¹p á»Ÿ NgÅ© HÃ nh SÆ¡n?",
    "answer": "[LLM generated answer]",
    "sources": [
        {hotel_id: 1, hotel_name: "Sheraton ÄÃ  Náºµng", ...},
        {hotel_id: 2, hotel_name: "InterContinental ÄÃ  Náºµng", ...},
        ... (5 sources)
    ]
}
```

---

## ğŸ”‘ Key Points

1. **RAG = Retrieval + Generation**: TÃ¬m kiáº¿m documents trÆ°á»›c, rá»“i má»›i generate answer
2. **k=5**: Láº¥y top 5 documents lÃ m context (tÄƒng tá»« 2 Ä‘á»ƒ chi tiáº¿t hÆ¡n)
3. **max_tokens=2048**: Cho phÃ©p response dÃ i vÃ  chi tiáº¿t hÆ¡n
4. **Embedding cache**: Tá»‘i Æ°u performance, trÃ¡nh re-embedding
5. **Smart chunking**: Chia nhá» hotel data nhÆ°ng preserve semantic meaning
6. **Location filtering**: Tá»± Ä‘á»™ng extract vÃ  filter theo location náº¿u cÃ³

